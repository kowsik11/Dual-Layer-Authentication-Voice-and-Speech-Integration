{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "from pathlib import Path\n",
        "from IPython.display import display, Audio\n",
        "\n",
        "\n",
        "DATASET_ROOT = \"F:\\\\16000_pcm_speeches\"\n",
        "# The folders in which we will put the audio samples and the noise samples\n",
        "AUDIO_SUBFOLDER = \"audio\"\n",
        "NOISE_SUBFOLDER = \"noise\"\n",
        "\n",
        "\n",
        "DATASET_AUDIO_PATH = os.path.join(DATASET_ROOT, AUDIO_SUBFOLDER)\n",
        "DATASET_NOISE_PATH = os.path.join(DATASET_ROOT, NOISE_SUBFOLDER)\n",
        "\n",
        "# Percentage of samples to use for validation\n",
        "VALID_SPLIT = 0.1\n",
        "\n",
        "# Seed to use when shuffling the dataset and the noise\n",
        "SHUFFLE_SEED = 43\n",
        "\n",
        "# The sampling rate to use.\n",
        "# This is the one used in all the audio samples.\n",
        "# We will resample all the noise to this sampling rate.\n",
        "# This will also be the output size of the audio wave samples\n",
        "# (since all samples are of 1 second long)\n",
        "SAMPLING_RATE = 16000\n",
        "\n",
        "# The factor to multiply the noise with according to:\n",
        "#   noisy_sample = sample + noise * prop * scale\n",
        "#      where prop = sample_amplitude / noise_amplitude\n",
        "SCALE = 0.5\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "EPOCHS = 24\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "# If folder `audio`, does not exist, create it, otherwise do nothing\n",
        "if os.path.exists(DATASET_AUDIO_PATH) is False:\n",
        "    os.makedirs(DATASET_AUDIO_PATH)\n",
        "\n",
        "# If folder `noise`, does not exist, create it, otherwise do nothing\n",
        "if os.path.exists(DATASET_NOISE_PATH) is False:\n",
        "    os.makedirs(DATASET_NOISE_PATH)\n",
        "\n",
        "for folder in os.listdir(DATASET_ROOT):\n",
        "    if os.path.isdir(os.path.join(DATASET_ROOT, folder)):\n",
        "        if folder in [AUDIO_SUBFOLDER, NOISE_SUBFOLDER]:\n",
        "            # If folder is `audio` or `noise`, do nothing\n",
        "            continue\n",
        "        elif folder in [\"other\", \"_background_noise_\"]:\n",
        "            # If folder is one of the folders that contains noise samples,\n",
        "            # move it to the `noise` folder\n",
        "            shutil.move(\n",
        "                os.path.join(DATASET_ROOT, folder),\n",
        "                os.path.join(DATASET_NOISE_PATH, folder),\n",
        "            )\n",
        "        else:\n",
        "            # Otherwise, it should be a speaker folder, then move it to\n",
        "            # `audio` folder\n",
        "            shutil.move(\n",
        "                os.path.join(DATASET_ROOT, folder),\n",
        "                os.path.join(DATASET_AUDIO_PATH, folder),\n",
        "            )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 0 files belonging to 0 directories\n"
          ]
        }
      ],
      "source": [
        "# Get the list of all noise files\n",
        "noise_paths = []\n",
        "for subdir in os.listdir(DATASET_NOISE_PATH):\n",
        "    subdir_path = Path(DATASET_NOISE_PATH) / subdir\n",
        "    if os.path.isdir(subdir_path):\n",
        "        noise_paths += [\n",
        "            os.path.join(subdir_path, filepath)\n",
        "            for filepath in os.listdir(subdir_path)\n",
        "            if filepath.endswith(\".wav\")\n",
        "        ]\n",
        "\n",
        "print(\n",
        "    \"Found {} files belonging to {} directories\".format(\n",
        "        len(noise_paths), len(os.listdir(DATASET_NOISE_PATH))\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sampling rate for E:\\SEM 7\\20CYS443 Biometries and Security\\Project\\dataset\\16000_pcm_speeches\\noise\\other\\exercise_bike.wav is incorrect. Ignoring it\n",
            "Sampling rate for E:\\SEM 7\\20CYS443 Biometries and Security\\Project\\dataset\\16000_pcm_speeches\\noise\\other\\pink_noise.wav is incorrect. Ignoring it\n",
            "Sampling rate for E:\\SEM 7\\20CYS443 Biometries and Security\\Project\\dataset\\16000_pcm_speeches\\noise\\_background_noise_\\10convert.com_Audience-Claps_daSG5fwdA7o.wav is incorrect. Ignoring it\n",
            "Sampling rate for E:\\SEM 7\\20CYS443 Biometries and Security\\Project\\dataset\\16000_pcm_speeches\\noise\\_background_noise_\\doing_the_dishes.wav is incorrect. Ignoring it\n",
            "Sampling rate for E:\\SEM 7\\20CYS443 Biometries and Security\\Project\\dataset\\16000_pcm_speeches\\noise\\_background_noise_\\dude_miaowing.wav is incorrect. Ignoring it\n",
            "Sampling rate for E:\\SEM 7\\20CYS443 Biometries and Security\\Project\\dataset\\16000_pcm_speeches\\noise\\_background_noise_\\running_tap.wav is incorrect. Ignoring it\n"
          ]
        },
        {
          "ename": "IndexError",
          "evalue": "tuple index out of range",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[7], line 37\u001b[0m\n\u001b[0;32m     32\u001b[0m         noises\u001b[38;5;241m.\u001b[39mextend(sample)\n\u001b[0;32m     33\u001b[0m noises \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mstack(noises)\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m noise files were split into \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m noise samples where each is \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m sec. long\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m---> 37\u001b[0m         \u001b[38;5;28mlen\u001b[39m(noise_paths), noises\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[43mnoises\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m SAMPLING_RATE\n\u001b[0;32m     38\u001b[0m     )\n\u001b[0;32m     39\u001b[0m )\n",
            "File \u001b[1;32mc:\\Users\\UDAY\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_shape.py:957\u001b[0m, in \u001b[0;36mTensorShape.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    955\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    956\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_v2_behavior:\n\u001b[1;32m--> 957\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dims\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    958\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    959\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdims[key]\n",
            "\u001b[1;31mIndexError\u001b[0m: tuple index out of range"
          ]
        }
      ],
      "source": [
        "command = (\n",
        "    \"for dir in `ls -1 \" + DATASET_NOISE_PATH + \"`; do \"\n",
        "    \"for file in `ls -1 \" + DATASET_NOISE_PATH + \"/$dir/*.wav`; do \"\n",
        "    \"sample_rate=`ffprobe -hide_banner -loglevel panic -show_streams \"\n",
        "    \"$file | grep sample_rate | cut -f2 -d=`; \"\n",
        "    \"if [ $sample_rate -ne 16000 ]; then \"\n",
        "    \"ffmpeg -hide_banner -loglevel panic -y \"\n",
        "    \"-i $file -ar 16000 temp.wav; \"\n",
        "    \"mv temp.wav $file; \"\n",
        "    \"fi; done; done\"\n",
        ")\n",
        "os.system(command)\n",
        "\n",
        "# Split noise into chunks of 16,000 steps each\n",
        "def load_noise_sample(path):\n",
        "    sample, sampling_rate = tf.audio.decode_wav(\n",
        "        tf.io.read_file(path), desired_channels=1\n",
        "    )\n",
        "    if sampling_rate == SAMPLING_RATE:\n",
        "        # Number of slices of 16000 each that can be generated from the noise sample\n",
        "        slices = int(sample.shape[0] / SAMPLING_RATE)\n",
        "        sample = tf.split(sample[: slices * SAMPLING_RATE], slices)\n",
        "        return sample\n",
        "    else:\n",
        "        print(\"Sampling rate for {} is incorrect. Ignoring it\".format(path))\n",
        "        return None\n",
        "\n",
        "noises = []\n",
        "for path in noise_paths:\n",
        "    sample = load_noise_sample(path)\n",
        "    if sample:\n",
        "        noises.extend(sample)\n",
        "noises = tf.stack(noises)\n",
        "\n",
        "print(\n",
        "    \"{} noise files were split into {} noise samples where each is {} sec. long\".format(\n",
        "        len(noise_paths), noises.shape[0], noises.shape[1] // SAMPLING_RATE\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def paths_and_labels_to_dataset(audio_paths, labels):\n",
        "    \"\"\"Constructs a dataset of audios and labels.\"\"\"\n",
        "    path_ds = tf.data.Dataset.from_tensor_slices(audio_paths)\n",
        "    audio_ds = path_ds.map(lambda x: path_to_audio(x))\n",
        "    label_ds = tf.data.Dataset.from_tensor_slices(labels)\n",
        "    return tf.data.Dataset.zip((audio_ds, label_ds))\n",
        "\n",
        "\n",
        "def path_to_audio(path):\n",
        "    \"\"\"Reads and decodes an audio file.\"\"\"\n",
        "    audio = tf.io.read_file(path)\n",
        "    audio, _ = tf.audio.decode_wav(audio, 1, SAMPLING_RATE)\n",
        "    return audio\n",
        "\n",
        "\n",
        "def add_noise(audio, noises=None, scale=0.5):\n",
        "    if noises is not None:\n",
        "        # Create a random tensor of the same size as audio ranging from\n",
        "        # 0 to the number of noise stream samples that we have.\n",
        "        tf_rnd = tf.random.uniform(\n",
        "            (tf.shape(audio)[0],), 0, noises.shape[0], dtype=tf.int32\n",
        "        )\n",
        "        noise = tf.gather(noises, tf_rnd, axis=0)\n",
        "\n",
        "        # Get the amplitude proportion between the audio and the noise\n",
        "        prop = tf.math.reduce_max(audio, axis=1) / tf.math.reduce_max(noise, axis=1)\n",
        "        prop = tf.repeat(tf.expand_dims(prop, axis=1), tf.shape(audio)[1], axis=1)\n",
        "\n",
        "        # Adding the rescaled noise to audio\n",
        "        audio = audio + noise * prop * scale\n",
        "\n",
        "    return audio\n",
        "\n",
        "\n",
        "def audio_to_fft(audio):\n",
        "    # Since tf.signal.fft applies FFT on the innermost dimension,\n",
        "    # we need to squeeze the dimensions and then expand them again\n",
        "    # after FFT\n",
        "    audio = tf.squeeze(audio, axis=-1)\n",
        "    fft = tf.signal.fft(\n",
        "        tf.cast(tf.complex(real=audio, imag=tf.zeros_like(audio)), tf.complex64)\n",
        "    )\n",
        "    fft = tf.expand_dims(fft, axis=-1)\n",
        "\n",
        "    # Return the absolute value of the first half of the FFT\n",
        "    # which represents the positive frequencies\n",
        "    return tf.math.abs(fft[:, : (audio.shape[1] // 2), :])\n",
        "\n",
        "\n",
        "# Get the list of audio file paths along with their corresponding labels\n",
        "\n",
        "class_names = os.listdir(DATASET_AUDIO_PATH)\n",
        "print(\"Our class names: {}\".format(class_names,))\n",
        "\n",
        "audio_paths = []\n",
        "labels = []\n",
        "for label, name in enumerate(class_names):\n",
        "    print(\"Processing speaker {}\".format(name,))\n",
        "    dir_path = Path(DATASET_AUDIO_PATH) / name\n",
        "    speaker_sample_paths = [\n",
        "        os.path.join(dir_path, filepath)\n",
        "        for filepath in os.listdir(dir_path)\n",
        "        if filepath.endswith(\".wav\")\n",
        "    ]\n",
        "    audio_paths += speaker_sample_paths\n",
        "    labels += [label] * len(speaker_sample_paths)\n",
        "\n",
        "print(\n",
        "    \"Found {} files belonging to {} classes.\".format(len(audio_paths), len(class_names))\n",
        ")\n",
        "\n",
        "# Shuffle\n",
        "rng = np.random.RandomState(SHUFFLE_SEED)\n",
        "rng.shuffle(audio_paths)\n",
        "rng = np.random.RandomState(SHUFFLE_SEED)\n",
        "rng.shuffle(labels)\n",
        "\n",
        "# Split into training and validation\n",
        "num_val_samples = int(VALID_SPLIT * len(audio_paths))\n",
        "print(\"Using {} files for training.\".format(len(audio_paths) - num_val_samples))\n",
        "train_audio_paths = audio_paths[:-num_val_samples]\n",
        "train_labels = labels[:-num_val_samples]\n",
        "\n",
        "print(\"Using {} files for validation.\".format(num_val_samples))\n",
        "valid_audio_paths = audio_paths[-num_val_samples:]\n",
        "valid_labels = labels[-num_val_samples:]\n",
        "\n",
        "# Create 2 datasets, one for training and the other for validation\n",
        "train_ds = paths_and_labels_to_dataset(train_audio_paths, train_labels)\n",
        "train_ds = train_ds.shuffle(buffer_size=BATCH_SIZE * 8, seed=SHUFFLE_SEED).batch(\n",
        "    BATCH_SIZE\n",
        ")\n",
        "\n",
        "valid_ds = paths_and_labels_to_dataset(valid_audio_paths, valid_labels)\n",
        "valid_ds = valid_ds.shuffle(buffer_size=32 * 8, seed=SHUFFLE_SEED).batch(32)\n",
        "\n",
        "\n",
        "# Add noise to the training set\n",
        "train_ds = train_ds.map(\n",
        "    lambda x, y: (add_noise(x, noises, scale=SCALE), y),\n",
        "    num_parallel_calls=tf.data.AUTOTUNE,\n",
        ")\n",
        "\n",
        "# Transform audio wave to the frequency domain using `audio_to_fft`\n",
        "train_ds = train_ds.map(\n",
        "    lambda x, y: (audio_to_fft(x), y), num_parallel_calls=tf.data.AUTOTUNE\n",
        ")\n",
        "train_ds = train_ds.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "valid_ds = valid_ds.map(\n",
        "    lambda x, y: (audio_to_fft(x), y), num_parallel_calls=tf.data.AUTOTUNE\n",
        ")\n",
        "valid_ds = valid_ds.prefetch(tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "history = model.fit(\n",
        "    train_ds,\n",
        "    epochs=EPOCHS,\n",
        "    validation_data=valid_ds,\n",
        "    callbacks=[earlystopping_cb, mdlcheckpoint_cb],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "model = load_model(\"model.keras\")  # Ensure that the model file exists and is compatible\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(model.evaluate(valid_ds))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        " def paths_to_dataset(audio_paths):\n",
        "    path_ds = tf.data.Dataset.from_tensor_slices(audio_paths)\n",
        "    return tf.data.Dataset.zip((path_ds))\n",
        "\n",
        "def predict(path, labels):\n",
        "    test = paths_and_labels_to_dataset(path, labels)\n",
        "\n",
        "\n",
        "    test = test.shuffle(buffer_size=BATCH_SIZE * 8, seed=SHUFFLE_SEED).batch(\n",
        "    BATCH_SIZE\n",
        "    )\n",
        "    test = test.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "\n",
        "    test = test.map(lambda x, y: (add_noise(x, noises, scale=SCALE), y))\n",
        "\n",
        "    for audios, labels in test.take(1):\n",
        "        ffts = audio_to_fft(audios)\n",
        "        y_pred = model.predict(ffts)\n",
        "        rnd = np.random.randint(0, 1, 1)\n",
        "        audios = audios.numpy()[rnd, :]\n",
        "        labels = labels.numpy()[rnd]\n",
        "        y_pred = np.argmax(y_pred, axis=-1)[rnd]\n",
        "\n",
        "    for index in range(1):\n",
        "            print(\n",
        "            \"Speaker:\\33{} {}\\33[0m\\tPredicted:\\33{} {}\\33[0m\".format(\n",
        "            \"[92m\",y_pred[index],\n",
        "                \"[92m\", y_pred[index]\n",
        "                )\n",
        "            )\n",
        "\n",
        "            print(\"Speaker Predicted:\",class_names[y_pred[index]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "SAMPLES_TO_DISPLAY = 10\n",
        "\n",
        "test_ds = paths_and_labels_to_dataset(valid_audio_paths, valid_labels)\n",
        "test_ds = test_ds.shuffle(buffer_size=BATCH_SIZE * 8, seed=SHUFFLE_SEED).batch(\n",
        "    BATCH_SIZE\n",
        ")\n",
        "\n",
        "test_ds = test_ds.map(lambda x, y: (add_noise(x, noises, scale=SCALE), y))\n",
        "\n",
        "for audios, labels in test_ds.take(1):\n",
        "    # Get the signal FFT\n",
        "    ffts = audio_to_fft(audios)\n",
        "    # Predict\n",
        "    y_pred = model.predict(ffts)\n",
        "    # Take random samples\n",
        "    rnd = np.random.randint(0, BATCH_SIZE, SAMPLES_TO_DISPLAY)\n",
        "    audios = audios.numpy()[rnd, :, :]\n",
        "    labels = labels.numpy()[rnd]\n",
        "    y_pred = np.argmax(y_pred, axis=-1)[rnd]\n",
        "\n",
        "    for index in range(SAMPLES_TO_DISPLAY):\n",
        "        # For every sample, print the true and predicted label\n",
        "        # as well as run the voice with the noise\n",
        "        print(\n",
        "            \"Speaker:\\33{} {}\\33[0m\\tPredicted:\\33{} {}\\33[0m\".format(\n",
        "                \"[92m\" if labels[index] == y_pred[index] else \"[91m\",\n",
        "                class_names[labels[index]],\n",
        "                \"[92m\" if labels[index] == y_pred[index] else \"[91m\",\n",
        "                class_names[y_pred[index]],\n",
        "            )\n",
        "        )\n",
        "        display(Audio(audios[index, :, :].squeeze(), rate=SAMPLING_RATE))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import soundfile as sf  # Library to load .wav files\n",
        "\n",
        "def load_audio(path):\n",
        "    \"\"\"Load an audio file and return it as a tensor.\"\"\"\n",
        "    audio_data, _ = sf.read(path.numpy())  # Convert tensor path to string and load the file\n",
        "    audio_data = tf.convert_to_tensor(audio_data, dtype=tf.float32)  # Convert to tensor\n",
        "    return tf.expand_dims(audio_data, axis=-1)  # Add an extra dimension for the channel\n",
        "\n",
        "def load_audio_wrapper(path):\n",
        "    \"\"\"A wrapper to use load_audio with tf.py_function.\"\"\"\n",
        "    audio = tf.py_function(load_audio, [path], tf.float32)  # Use tf.py_function for non-TensorFlow ops\n",
        "    return audio\n",
        "\n",
        "def paths_to_dataset(audio_paths):\n",
        "    \"\"\"Convert a list of audio paths into a dataset of loaded audio data.\"\"\"\n",
        "    path_ds = tf.data.Dataset.from_tensor_slices(audio_paths)\n",
        "    audio_ds = path_ds.map(lambda path: load_audio_wrapper(path))  # Use the wrapper function\n",
        "    return audio_ds\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import librosa\n",
        "import soundfile as sf\n",
        "import numpy as np\n",
        "\n",
        "def load_and_pad_audio(file_path, target_length=16000, sr=16000):\n",
        "    # Load audio file with librosa\n",
        "    audio_data, sample_rate = librosa.load(file_path, sr=sr)\n",
        "\n",
        "    # If the audio is shorter than the target length, pad with zeros\n",
        "    if len(audio_data) < target_length:\n",
        "        padding = target_length - len(audio_data)\n",
        "        audio_data = np.pad(audio_data, (0, padding), 'constant')\n",
        "\n",
        "    # If the audio is longer, truncate it\n",
        "    elif len(audio_data) > target_length:\n",
        "        audio_data = audio_data[:target_length]\n",
        "\n",
        "    return audio_data, sample_rate\n",
        "\n",
        "# Load, pad, and save the audio\n",
        "audio_data, sample_rate = load_and_pad_audio(\"/content/qwer.wav\")\n",
        "sf.write(\"/content/qwer_padded.wav\", audio_data, sample_rate)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def predict(audio_path, labels, threshold=0.94):\n",
        "    \"\"\"Process and predict the speaker.\"\"\"\n",
        "    test = paths_to_dataset([audio_path])  # Load the audio\n",
        "\n",
        "    test = test.batch(1)  # Process one audio at a time\n",
        "    test = test.map(lambda x: add_noise(x, noises, scale=SCALE))  # Add noise if needed\n",
        "    test = test.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "    for audios in test.take(1):\n",
        "        ffts = audio_to_fft(audios)  # Convert audio to FFT\n",
        "        y_pred = model.predict(ffts)  # Make predictions\n",
        "        y_pred_class = np.argmax(y_pred, axis=-1)[0]  # Get the predicted class\n",
        "        y_pred_confidence = np.max(y_pred)  # Get the confidence score\n",
        "\n",
        "    # Check if the prediction confidence is above the threshold\n",
        "    if y_pred_confidence >= threshold:\n",
        "        return class_names[y_pred_class], y_pred_confidence\n",
        "    else:\n",
        "        return \"Unknown\", y_pred_confidence\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def authenticate_user(audio_path, known_speakers, threshold=0.94):\n",
        "    \"\"\"\n",
        "    Authenticate the user based on their voice.\n",
        "\n",
        "    Parameters:\n",
        "    audio_path (str): Path to the audio file to authenticate.\n",
        "    known_speakers (list): List of known speaker names (the 5 trained individuals).\n",
        "    threshold (float): Confidence threshold for authentication.\n",
        "\n",
        "    Returns:\n",
        "    str: \"Authenticated\" if the speaker is recognized, otherwise \"Access Denied\".\n",
        "    \"\"\"\n",
        "    predicted_speaker, confidence = predict(audio_path, known_speakers, threshold)\n",
        "\n",
        "    # Check if the predicted speaker is in the list of known speakers\n",
        "    if predicted_speaker in known_speakers:\n",
        "        return f\"Authenticated: {predicted_speaker} with confidence {confidence:.2f}\"\n",
        "    else:\n",
        "        return f\"Access Denied: Confidence {confidence:.2f}\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "trained_speakers = [\"Benjamin_Netanyau\", \"Jens_Stoltenberg\", \"Julia_Gillard\", \"Margaret_Thatcher\", \"Nelson_Mandela\",\"unknown\"]\n",
        "\n",
        "# Example usage\n",
        "audio_file_path = \"/content/qwer_padded.wav\"\n",
        "result = authenticate_user(audio_file_path, trained_speakers)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "audio_file_path = \"/content/drive/MyDrive/16000_pcm_speeches/audio/Jens_Stoltenberg/10.wav\"\n",
        "result = authenticate_user(audio_file_path, trained_speakers)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "y_pred = model.predict(valid_ds)\n",
        "\n",
        "# Get the true labels\n",
        "y_true = np.array(valid_labels)\n",
        "\n",
        "# Generate confusion matrix\n",
        "cm = confusion_matrix(y_true, np.argmax(y_pred, axis=1))\n",
        "\n",
        "# Print the confusion matrix\n",
        "print(cm)\n",
        "print(\"\")\n",
        "# Plot the confusion matrix\n",
        "plt.imshow(cm, cmap='Blues')\n",
        "plt.colorbar()\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
